# 目标检测和分割任务

## 概述

图像分类是计算机视觉最基本的任务之一，但是在图像分类的基础上，还有更复杂和有意思的任务，如目标检测，物体定位，图像分割等。其中目标检测是一件比较实际的且具有挑战性的计算机视觉任务，其可以看成图像分类与定位的结合，给定一张图片，目标检测系统要能够识别出图片的目标并给出其位置，由于图片中目标数是不定的，且要给出目标的精确位置，目标检测相比分类任务更复杂。、

近几年来，目标检测算法取得了很大的突破。比较流行的算法可以分为两类，一类是基于Region Proposal的R-CNN系算法（R-CNN，Fast R-CNN, Faster R-CNN），它们是two-stage的，需要先使用启发式方法（selective search）或者CNN网络（RPN）产生Region Proposal，然后再在Region Proposal上做分类与回归。而另一类是Yolo，SSD这类one-stage算法，其仅仅使用一个CNN网络直接预测不同目标的类别与位置。第一类方法是准确度高一些，但是速度慢，但是第二类算法是速度快，但是准确性要低一些。

### 几种任务比较

- 目标定位与识别最简单，只有一个目标。
- 目标检测其次，因为它有多个目标，每一个都要识别。
- 语义分割最难，不仅有多个目标，还要明确标出分界线。
- 常规的识别任务也就是分类问题，比目标定位与识别更简单，因为目标定位与识别不仅返回label，还要返回位置。

对于第二个和第三个任务，可以以某一个很小的方框依次扫描整个图，从每一个采集到的图像中，送到识别器中，看是否是想要的。然后把方框逐渐变大，再从头到尾扫描。

### 常用的目标检测模型

- YOLO（You Only Look Once）系列
- 最新的SAM（segment anything model）
- EfficientDet
- RetinaNet
- Faster R-CNN
- Mask R-CNN
- CenterNet
- DETR
- Cascade R-CNN

本文主要使用YOLO系列模型和SAM模型，并介绍如何使用这些模型以及微调。

### SOTA模型

SOTA，全称「state-of-the-art」，用于描述机器学习中取得某个任务上当前最优效果的模型。例如在图像分类任务上，某个模型在常用的数据集（如 ImageNet）上取得了当前最优的性能表现，我们就可以说这个模型达到了 SOTA。

## YOLO系列

### 基本介绍

官网github地址：<https://github.com/ultralytics/ultralytics>，通过`pip install ultralytics`安装，该包中包含了所有的YOLO系列模型，甚至最新SAM模型都可以从这个库中找到。

YOLO系列最新是YOLOv8模型，YOLOv8 是一个 SOTA 模型，它建立在以前 YOLO 版本的成功基础上，并引入了新的功能和改进，以进一步提升性能和灵活性。具体创新包括一个新的骨干网络、一个新的 Ancher-Free 检测头和一个新的损失函数，可以在从 CPU 到 GPU 的各种硬件平台上运行。

ultralytics 开源库定位为算法框架，而非某一个特定算法，一个主要特点是可扩展性。主要又两个优点：融合众多当前 SOTA 技术于一体，未来将支持其他 YOLO 系列以及 YOLO 之外的更多算法。

YOLOv8 相比 YOLOv5 精度提升非常多，但是 N/S/M 模型相应的参数量和 FLOPs 都增加了不少，从上图也可以看出相比 YOLOv5 大部分模型推理速度变慢了。

### YOLOv8特性和改动

YOLOv8 主要参考了最近提出的诸如 YOLOX、YOLOv6、YOLOv7 和 PPYOLOE 等算法的相关设计，本身的创新点不多，偏向工程实践，主推的还是 ultralytics 这个框架本身。

1. 提供了一个全新的 SOTA 模型，包括 P5 640 和 P6 1280 分辨率的目标检测网络和基于 YOLACT 的实例分割模型。和 YOLOv5 一样，基于缩放系数也提供了 N/S/M/L/X 尺度的不同大小模型，用于满足不同场景需求
2. 骨干网络和 Neck 部分可能参考了 YOLOv7 ELAN 设计思想，将 YOLOv5 的 C3 结构换成了梯度流更丰富的 C2f 结构，并对不同尺度模型调整了不同的通道数，属于对模型结构精心微调，不再是无脑一套参数应用所有模型，大幅提升了模型性能。不过这个 C2f 模块中存在 Split 等操作对特定硬件部署没有之前那么友好了
3. Head 部分相比 YOLOv5 改动较大，换成了目前主流的解耦头结构，将分类和检测头分离，同时也从 Anchor-Based 换成了 Anchor-Free
4. Loss 计算方面采用了 TaskAlignedAssigner 正样本分配策略，并引入了 Distribution Focal Loss
5. 训练的数据增强部分引入了 YOLOX 中的最后 10 epoch 关闭 Mosiac 增强的操作，可以有效地提升精度

模型结构参考：<https://github.com/open-mmlab/mmyolo/blob/dev/configs/yolov8/README.md>。

### YOLOv1

#### 简介

在YOLOv1提出之前，R-CNN系列算法在目标检测领域独占鳌头。R-CNN系列检测精度高，但是由于其网络结构是双阶段（two-stage）的特点，使得它的检测速度不能满足实时性，饱受诟病。为了打破这一僵局，设计一种速度更快的目标检测器是大势所趋。

2016年，Joseph Redmon、Santosh Divvala、Ross Girshick等人提出了一种单阶段（one-stage）的目标检测网络。它的检测速度非常快，每秒可以处理45帧图片，能够轻松地实时运行。由于其速度之快和其使用的特殊方法，作者将其取名为：You Only Look Once（也就是我们常说的YOLO的全称），并将该成果发表在了CVPR 2016上，从而引起了广泛地关注。

YOLO 的核心思想就是把目标检测转变成一个回归问题，利用整张图作为网络的输入，仅仅经过一个神经网络，得到bounding box（边界框） 的位置及其所属的类别。

#### 网络结构

![YOLOv1网络结构](https://img-blog.csdnimg.cn/20200722170142957.png)

YOLOv1网路结构非常明晰，是一种传统的one-stage的卷积神经网络：

- 网络输入：448×448×3的彩色图片。
- 中间层：由若干卷积层和最大池化层组成，用于提取图片的抽象特征。
- 全连接层：由两个全连接层组成，用来预测目标的位置和类别概率值。
- 网络输出：7×7×30的预测结果。

#### 实现细节

YOLOv1采用“分而治之”的策略，将一张图片平均分成7×7个网格，每个网格分别负责预测中心点落在该网格内的目标，这49个网格就相当于是目标的感兴趣区域，而不需要像`Faster R-CNN`通过一个RPN来获得目标的感兴趣区域。

1. 将一幅图像分成 S×S个网格（grid cell），如果某个 object 的中心落在这个网格中，则这个网格就负责预测这个object。
2. 每个网格要预测 B 个bounding box，每个 bounding box 要预测 (x, y, w, h) 和 confidence 共5个值。
3. 每个网格还要预测一个类别信息，记为 C 个类。
4. 总的来说，S×S 个网格，每个网格要预测 B个bounding box ，还要预测 C 个类。网络输出就是一个 S × S × (5×B+C) 的张量。

每个网格预测2个Box（Box1和Box2），20个类别。所以实际上，S=7，B=2，C=20。那么网络输出的shape也就是：7×7×30。

![YOLOv1损失函数](https://img-blog.csdnimg.cn/20200722180056692.png)

- 损失由三部分组成，分别是：坐标预测损失、置信度预测损失、类别预测损失。
- 使用差方和误差，w和h在进行误差计算的时候取的是它们的平方根，原因是对不同大小的bounding box预测中，相比于大bounding box预测偏一点，小box预测偏一点更不能忍受。而差方和误差函数中对同样的偏移loss是一样。 为了缓和这个问题，作者用了一个比较取巧的办法，就是将bounding box的w和h取平方根代替原本的w和h。
- 定位误差比分类误差更大，所以增加对定位误差的惩罚，使$ λ_{coord}=5 $
- 在每个图像中，许多网格单元不包含任何目标。训练时就会把这些网格里的框的“置信度”分数推到零，这往往超过了包含目标的框的梯度。从而可能导致模型不稳定，训练早期发散。因此要减少了不包含目标的框的置信度预测的损失，使$ λ_{noobj}=0.5 $

#### 性能表现

（1）优点：

- YOLO检测速度非常快。标准版本的YOLO可以每秒处理 45 张图像；YOLO的极速版本每秒可以处理150帧图像。这就意味着 YOLO 可以以小于 25 毫秒延迟，实时地处理视频。对于欠实时系统，在准确率保证的情况下，YOLO速度快于其他方法。
- YOLO 实时检测的平均精度是其他实时监测系统的两倍。
- 迁移能力强，能运用到其他的新的领域（比如艺术品目标检测）。

（2）局限：

- YOLO对相互靠近的物体，以及很小的群体检测效果不好，这是因为一个网格只预测了2个框，并且都只属于同一类。
- 由于损失函数的问题，定位误差是影响检测效果的主要原因，尤其是大小物体的处理上，还有待加强。（因为对于小的bounding boxes，small error影响更大）
- YOLO对不常见的角度的目标泛化性能偏弱。

### YOLOv2

#### 简介

2017年，作者 Joseph Redmon 和 Ali Farhadi 在 YOLOv1 的基础上，进行了大量改进，提出了 YOLOv2 和 YOLO9000。重点解决YOLOv1召回率和定位精度方面的不足。

YOLOv2 是一个先进的目标检测算法，比其它的检测器检测速度更快。除此之外，该网络可以适应多种尺寸的图片输入，并且能在检测精度和速度之间进行很好的权衡。

相比于YOLOv1是利用全连接层直接预测Bounding Box的坐标，YOLOv2借鉴了Faster R-CNN的思想，引入Anchor机制。利用K-means聚类的方法在训练集中聚类计算出更好的Anchor模板，大大提高了算法的召回率。同时结合图像细粒度特征，将浅层特征与深层特征相连，有助于对小尺寸目标的检测。

YOLO9000 使用 WorldTree 来混合来自不同资源的训练数据，并使用联合优化技术同时在ImageNet和COCO数据集上进行训练，能够实时地检测超过9000种物体。由于 YOLO9000 的主要检测网络还是YOLOv2，所以这部分以讲解应用更为广泛的YOLOv2为主。

#### 网络结构

YOLOv2 采用 Darknet-19 作为特征提取网络，其整体结构如下：

![YOLOv2网络结构](https://img-blog.csdnimg.cn/20200728181645170.png)

- 与VGG相似，使用了很多3×3卷积核；并且每一次池化后，下一层的卷积核的通道数 = 池化输出的通道 × 2。
- 在每一层卷积后，都增加了批量标准化（Batch Normalization）进行预处理。
- 采用了降维的思想，把1×1的卷积置于3×3之间，用来压缩特征。
- 在网络最后的输出增加了一个global average pooling层。
- 整体上采用了19个卷积层，5个池化层。

Darknet-19 与 YOLOv1、VGG16网络进行对比：
 
- VGG-16： 大多数检测网络框架都是以VGG-16作为基础特征提取器，它功能强大，准确率高，但是计算复杂度较大，所以速度会相对较慢。因此YOLOv2的网络结构将从这方面进行改进。
- YOLOv1： 基于GoogLeNet的自定义网络（具体看上周报告），比VGG-16的速度快，但是精度稍不如VGG-16。
- Darknet-19： 速度方面，处理一张图片仅需要55.8亿次运算，相比于VGG306.9亿次，速度快了近6倍。精度方面，在ImageNet上的测试精度为：top1准确率为72.9%，top5准确率为91.2%。

#### 改进方法

- Batch Normalization
  - BN 对数据进行预处理（统一格式、均衡化、去噪等）能够大大提高训练速度，提升训练效果。
  - YOLOv2 对每一层输入的数据都进行批量标准化，这样网络就不需要每层都去学数据的分布，收敛会变得更快。
  - 在卷积或池化之后，激活函数之前，对每个数据输出进行标准化。
- 引入 Anchor Box 机制
  - 通过提前筛选得到的具有代表性先验框Anchors，使得网络在训练时更容易收敛。
  - 预测边界框中心点相对于该网格左上角坐标(Cx, Cy)，的相对偏移量，同时为了将bounding box的中心点约束在当前网格中，使用 sigmoid 函数将tx，ty归一化处理，将值约束在0-1，这使得模型训练更稳定。
- Convolution With Anchor Boxes
  - 首先将YOLOv1网络的FC层和最后一个Pooling层去掉，使得最后的卷积层的输出可以有更高的分辨率特征
  - 然后缩减网络，用416×416大小的输入代替原来的448×448，使得网络输出的特征图有奇数大小的宽和高，进而使得每个特征图在划分单元格的时候只有一个中心单元格（Center Cell）。YOLOv2通过5个Pooling层进行下采样，得到的输出是13×13的像素特征。
  - 借鉴Faster R-CNN，YOLOv2通过引入Anchor Boxes，预测Anchor Box的偏移值与置信度，而不是直接预测坐标值。
  - 采用Faster R-CNN中的方式，每个Cell可预测出9个Anchor Box，共13×13×9=1521个（YOLOv2确定Anchor Boxes的方法见是维度聚类，每个Cell选择5个Anchor Box）。比YOLOv1预测的98个bounding box 要多很多，因此在定位精度方面有较好的改善。
- 聚类方法选择Anchors
  - 使用 K-means 聚类方法得到 Anchor Box 的大小，选择具有代表性的尺寸的Anchor Box进行一开始的初始化
  - 传统的K-means聚类方法使用标准的欧氏距离作为距离度量，这意味着大的box会比小的box产生更多的错误。因此这里使用其他的距离度量公式。
- Fine-Grained Features：细粒度特征，可理解为不同层之间的特征融合

#### 性能表现

在VOC2007数据集上进行测试，YOLOv2在速度为67fps时，精度可以达到76.8的mAP；在速度为40fps时，精度可以达到78.6 的mAP 。可以很好的在速度和精度之间进行权衡。下图是YOLOv1在加入各种改进方法后，检测性能的改变。可见在经过多种改进方法后，YOLOv2在原基础上检测精度具有很大的提升！

### YOLOv3

#### 简介

2018年，作者 Redmon 又在 YOLOv2 的基础上做了一些改进。特征提取部分采用darknet-53网络结构代替原来的darknet-19，利用特征金字塔网络结构实现了多尺度检测，分类方法使用逻辑回归代替了softmax，在兼顾实时性的同时保证了目标检测的准确性。

从YOLOv1到YOLOv3，每一代性能的提升都与backbone（骨干网络）的改进密切相关。在YOLOv3中，作者不仅提供了darknet-53，还提供了轻量级的tiny-darknet。如果你想检测精度与速度兼具，可以选择darknet-53作为backbone；如果你希望达到更快的检测速度，精度方面可以妥协，那么tiny-darknet是你很好的选择。总之，YOLOv3的灵活性使得它在实际工程中得到很多人的青睐！

#### 网络结构

相比于 YOLOv2 的 骨干网络，YOLOv3 进行了较大的改进。借助残差网络的思想，YOLOv3 将原来的 darknet-19 改进为darknet-53。论文中给出的整体结构如下：

![YOLOv3网络结构](https://img-blog.csdnimg.cn/20200729120619887.png)

Darknet-53主要由1×1和3×3的卷积层组成，每个卷积层之后包含一个批量归一化层和一个Leaky ReLU，加入这两个部分的目的是为了防止过拟合。卷积层、批量归一化层以及Leaky ReLU共同组成Darknet-53中的基本卷积单元DBL。因为在Darknet-53中共包含53个这样的DBL，所以称其为Darknet-53。

![darknet-53的网络结构](https://img-blog.csdnimg.cn/2020072912091262.png)

- DBL： 一个卷积层、一个批量归一化层和一个Leaky ReLU组成的基本卷积单元。
- res unit： 输入通过两个DBL后，再与原输入进行add；这是一种常规的残差单元。残差单元的目的是为了让网络可以提取到更深层的特征，同时避免出现梯度消失或爆炸。
- resn： 其中的n表示n个res unit；所以 resn = Zero Padding + DBL + n × res unit 。
- concat： 将darknet-53的中间层和后面的某一层的上采样进行张量拼接，达到多尺度特征融合的目的。这与残差层的add操作是不一样的，拼接会扩充张量的维度，而add直接相加不会导致张量维度的改变。
- Y1、Y2、Y3： 分别表示YOLOv3三种尺度的输出。

与darknet-19对比可知，darknet-53主要做了如下改进：

- 没有采用最大池化层，转而采用步长为2的卷积层进行下采样。
- 为了防止过拟合，在每个卷积层之后加入了一个BN层和一个Leaky ReLU。
- 引入了残差网络的思想，目的是为了让网络可以提取到更深层的特征，同时避免出现梯度消失或爆炸。
- 将网络的中间层和后面某一层的上采样进行张量拼接，达到多尺度特征融合的目的。

#### 改进之处

YOLOv3最大的改进之处还在于网络结构的改进，由于上面已经讲过。因此下面主要对其它改进方面进行介绍：

- 设计了3种不同尺度的网络输出Y1、Y2、Y3 **预测不同尺度的目标**
- 置信度损失和类别预测均由原来的sum-square error改为了交叉熵的损失计算方法
- YOLOv3在类别预测方面将YOLOv2的单标签分类改进为多标签分类，在网络结构中将YOLOv2中用于分类的softmax层修改为逻辑分类器

#### 性能表现

YOLOv3达到了与当前先进检测器的同样的水平。检测精度最高的是单阶段网络RetinaNet，但是YOLOv3的推理速度比RetinaNet快得多。

![性能表现](https://img-blog.csdnimg.cn/20200729142220141.png)

### YOLOv4

#### 简介

2020年YOLO系列的作者Redmon在推特上发表声明，出于道德方面的考虑，从此退出CV界。听到此消息的我，为之震惊！本以为YOLOv3已经是YOLO系列的终局之战。没想到就在今年，Alexey Bochkovskiy等人与Redmon取得联系，正式将他们的研究命名为YOLOv4。

YOLOv4对深度学习中一些常用Tricks进行了大量的测试，最终选择了这些有用的Tricks：WRC、CSP、CmBN、SAT、 Mish activation、Mosaic data augmentation、CmBN、DropBlock regularization 和 CIoU loss。

YOLOv4在传统的YOLO基础上，加入了这些实用的技巧，实现了检测速度和精度的最佳权衡。实验表明，在Tesla V100上，对MS COCO数据集的实时检测速度达到65 FPS，精度达到43.5%AP。

YOLOv4的独到之处在于：

- 是一个高效而强大的目标检测网咯。它使我们每个人都可以使用 GTX 1080Ti 或 2080Ti 的GPU来训练一个超快速和精确的目标检测器。这对于买不起高性能显卡的我们来说，简直是个福音！
- 在论文中，验证了大量先进的技巧对目标检测性能的影响，真的是非常良心!
- 对当前先进的目标检测方法进行了改进，使之更有效，并且更适合在单GPU上训练；这些改进包括CBN、PAN、SAM等。

#### 网络结构

最简单清晰的表示： YOLOv4 = CSPDarknet53（主干） + SPP附加模块（颈） + PANet路径聚合（颈） + YOLOv3（头部）

![YOLOv4网络结构](https://img-blog.csdnimg.cn/20200730004622368.png)

- 在YOLOv4中，将原来的Darknet53结构换为了CSPDarknet53作为主干网络，[CSPNet](https://arxiv.org/pdf/1911.11929.pdf)全称是Cross Stage Partial Network，主要目的是能够实现更丰富的梯度组合，同时减少计算量
- 使用MIsh激活函数代替了原来的Leaky ReLU
- 引入[SPP](https://link.springer.com/content/pdf/10.1007/978-3-319-10578-9_23.pdf)显著地增加了感受野，分离出了最重要的上下文特征，并且几乎不会降低的YOLOv4运行速度，分别利用四个不同尺度的最大池化对上层输出的feature map进行处理。最大池化的池化核大小分别为13x13、9x9、5x5、1x1，其中1x1就相当于不处理。
- 使用[PANet](https://arxiv.org/pdf/1803.01534.pdf)代替YOLOv3中的FPN作为参数聚合的方法，针对不同的检测器级别从不同的主干层进行参数聚合。并且对原PANet方法进行了修改, 使用张量连接(concat)代替了原来的捷径连接(shortcut connection)。
- 继承了YOLOv3的Head进行多尺度预测，提高了对不同size目标的检测性能

#### 各种Tricks总结

作者将所有的Tricks可以分为两类：

- 在不增加推理成本的前提下获得更好的精度，而只改变训练策略或只增加训练成本的方法，作着称之为 “免费包”（Bag of freebies）
- 只增加少量推理成本但能显著提高目标检测精度的插件模块和后处理方法，称之为“特价包”（Bag of specials）

- 免费包
  - 常用的数据增强方法：
    - 随机缩放
    - 翻转、旋转
    - 图像扰动、加噪声、遮挡
    - 改变亮度、对比对、饱和度、色调
    - 随机裁剪（random crop）
    - 随机擦除（random erase）
    - Cutout
    - MixUp
    - CutMix
  - 常见的正则化方法有：
    - DropOut
    - DropConnect
    - DropBlock
  - 平衡正负样本的方法有：
    - Focal loss
    - OHEM(在线难分样本挖掘)
  - 回归损失方面的改进：
    - GIOU
    - DIOU
    - CIoU
- 特价包
  - 增大感受野技巧：
    - SPP
    - ASPP
    - RFB
  - 注意力机制：
    - Squeeze-and-Excitation (SE)
    - Spatial Attention Module (SAM)
  - 特征融合集成：
    - FPN
    - SFAM
    - ASFF
    - BiFPN （出自于大名鼎鼎的EfficientDet）
  - 更好的激活函数：
    - ReLU
    - LReLU
    - PReLU
    - ReLU6
    - SELU
    - Swish
    - hard-Swish
  - 后处理非极大值抑制算法：
    - soft-NMS
    - DIoU NMS

#### 改进方法

除了下面已经提到的各种Tricks，为了使目标检测器更容易在单GPU上训练，作者也提出了5种改进方法：

- Mosaic：一种新的数据增强方法，借鉴了CutMix数据增强方式的思想。CutMix数据增强方式利用两张图片进行拼接，但是Mosaic使利用四张图片进行拼接。优点：拥有丰富检测目标的背景，并且在BN计算的时候一次性会处理四张图片！
- SAT：一种自对抗训练数据增强方法，这一种新的对抗性训练方式。在第一阶段，神经网络改变原始图像而不改变网络权值。以这种方式，神经网络对自身进行对抗性攻击，改变原始图像，以制造图像上没有所需对象的欺骗。在第二阶段，用正常的方法训练神经网络去检测目标。
- CmBN：CmBN的全称是Cross mini-Batch Normalization，定义为跨小批量标准化（CmBN）。CmBN 是 CBN 的改进版本，它用来收集一个batch内多个mini-batch内的统计数据。
- 修改过的SAM：作者在原SAM（Spatial Attention Module）方法上进行了修改，将SAM从空间注意修改为点注意
- 修改过的PAN：作者对原PAN(Path Aggregation Network)方法进行了修改, 使用张量连接(concat)代替了原来的快捷连接(shortcut connection)。

#### 性能表现

如下图所示，在COCO目标检测数据集上，对当前各种先进的目标检测器进行了测试。可以发现，YOLOv4的检测速度比EfficientDet快两倍，性能相当。同时，将YOLOv3的AP和FPS分别提高10%和12%，吊打YOLOv3!

![YOLOv4性能表现](https://img-blog.csdnimg.cn/20200729150006862.png)

综合以上分析，总结出YOLOv4带给我们的优点有：

- 与其它先进的检测器相比，对于同样的精度，YOLOv4更快（FPS）；对于同样的速度，YOLOv4更准（AP）。
- YOLOv4能在普通的GPU上训练和使用，比如GTX 1080Ti和GTX 2080Ti等。
- 论文中总结了各种Tricks（包括各种BoF和BoS），给我们启示，选择合适的Tricks来提高自己的检测器性能。

### YOLOv5

#### 简介

YOLOv5是一个在COCO数据集上预训练的物体检测架构和模型系列，它代表了Ultralytics对未来视觉AI方法的开源研究，其中包含了经过数千小时的研究和开发而形成的经验教训和最佳实践。

YOLOv5是YOLO系列的一个延申，您也可以看作是基于YOLOv3、YOLOv4的改进作品。YOLOv5没有相应的论文说明，但是作者在Github上积极地开放源代码，通过对源码分析，我们也能很快地了解YOLOv5的网络架构和工作原理。

#### 网络结构

YOLOv5官方代码中，一共给出了5个版本，分别是 YOLOv5n、YOLOv5s、YOLOv5m、YOLOv5l、YOLO5x 五个模型。这些不同的变体使得YOLOv5能很好的在精度和速度中权衡，方便用户选择。

![YOLOv5网络结构](https://img-blog.csdnimg.cn/408486b724aa4b46b4a785cfd6c92216.png)

- Input：和YOLOv4一样，对输入的图像进行Mosaic数据增强
- Backbone：Focus结构、CSP结构，通过降维和压缩输入特征图，从而减少计算量和提高感受野，同时提高目标检测的精度和模型的表达能力
- Neck：FPN+PAN结构，进行丰富的特征融合，这一部分和YOLOv4的结构相同
- Head：对于网络的输出，遵循YOLO系列的一贯做法，采用的是耦合的Head

#### 改进方法

- 自适应锚框计算：每次训练时，自适应的计算不同训练集中的最佳锚框值
- 自适应灰度填充：采用了灰度填充的方式统一输入尺寸，避免了目标变形的问题
- 损失函数：分类用交叉熵损失函数（BEC Loss），边界框回归用 CIoU Loss，CIOU将目标与anchor之间的中心距离，重叠率、尺度以及惩罚项都考虑进去，使得目标框回归变得更加稳定，不会像IoU和GIoU一样出现训练过程中发散等问题。而惩罚因子把预测框长宽比拟合目标框的长宽比考虑进去。

#### 性能表现

在COCO数据集上，当输入原图的尺寸是：640x640时，YOLOv5的5个不同版本的模型的检测数据如下：

![YOLOv5性能表现](https://img-blog.csdnimg.cn/9a02f0d8037a4fcb80a0711ccdc53dea.png#pic_center)

### YOLOv8

#### 简介

OLOv8 与YOLOv5出自同一个团队，是一款前沿、最先进（SOTA）的模型，基于先前 YOLOv5版本的成功，引入了新功能和改进，进一步提升性能和灵活性。YOLOv8 设计快速、准确且易于使用，使其成为各种物体检测与跟踪、实例分割、图像分类和姿态估计任务的绝佳选择。

#### 网络结构

整体结构上与YOLOv5类似： CSPDarknet（主干） + PAN-FPN（颈） + Decoupled-Head（输出头部），但是在各模块的细节上有一些改进，并且整体上是基于anchor-free的思想，这与yolov5也有着本质上的不同。

![YOLOv8网络结构](https://img-blog.csdnimg.cn/17635ab49fc4489fae21b8ad10509686.png)

#### 改进方法

- Backbone
  - 使用的依旧是CSP的思想，不过YOLOv5中的C3模块被替换成了C2f模块，实现了进一步的轻量化，同时YOLOv8依旧使用了YOLOv5等架构中使用的SPPF模块；
  - 针对C3模块，其主要是借助CSPNet提取分流的思想，同时结合残差结构的思想，设计了所谓的C3 Block，这里的CSP主分支梯度模块为BottleNeck模块，也就是所谓的残差模块。同时堆叠的个数由参数n来进行控制，也就是说不同规模的模型，n的值是有变化的。
  - C2f模块就是参考了C3模块以及ELAN（来自YOLOv7）的思想进行的设计，让YOLOv8可以在保证轻量化的同时获得更加丰富的梯度流信息。
  YOLOv7通过并行更多的梯度流分支，设计ELAN模块可以获得更丰富的梯度信息，进而或者更高的精度和更合理的延迟。
- PAN-FPN
  - 毫无疑问YOLOv8依旧使用了PAN的思想，不过通过对比YOLOv5与YOLOv8的结构图可以看到，YOLOv8将YOLOv5中PAN-FPN上采样阶段中的卷积结构删除了，同时也将C3模块替换为了C2f模块；
- Decoupled-Head
  - 与YOLOX类似，采用了解耦的输出头部，分别进行类别和边界框的回归学习
- Anchor-Free 
  - YOLOv8抛弃了以往的Anchor-Base，使用了Anchor-Free的思想；
- 损失函数
  - YOLOv8使用VFL Loss作为分类损失，使用DFL Loss+CIOU Loss作为分类损失；
  - VFL主要改进是提出了非对称的加权操作，FL和QFL都是对称的。而非对称加权的思想来源于论文PISA，该论文指出首先正负样本有不平衡问题，即使在正样本中也存在不等权问题，因为mAP的计算是主正样本
  - q是label，正样本时候q为bbox和gt的IoU，负样本时候q=0，当为正样本时候其实没有采用FL，而是普通的BCE，只不过多了一个自适应IoU加权，用于突出主样本。而为负样本时候就是标准的FL了。可以明显发现VFL比QFL更加简单，主要特点是正负样本非对称加权、突出正样本为主样本。
  - 针对这里的DFL（Distribution Focal Loss），其主要是将框的位置建模成一个 general distribution，让网络快速的聚焦于和目标位置距离近的位置的分布；DFL 能够让网络更快地聚焦于目标 y 附近的值，增大它们的概率。
  - DFL的含义是以交叉熵的形式去优化与标签y最接近的一左一右2个位置的概率，从而让网络更快的聚焦到目标位置的邻近区域的分布；也就是说学出来的分布理论上是在真实浮点坐标的附近，并且以线性插值的模式得到距离左右整数坐标的权重。
- 样本匹配
  - YOLOv8抛弃了以往的IOU匹配或者边长比例的分配方式，而是使用了Task-Aligned Assigner匹配方式。

#### 性能表现

YOLOv8 的检测、分割和姿态模型在 COCO 数据集上进行预训练，而分类模型在 ImageNet 数据集上进行预训练。在首次使用时，模型会自动从最新的 Ultralytics 发布版本中下载。

YOLOv8共提供了5中不同大小的模型选择，方便开发者在性能和精度之前进行平衡。以下以YOLOv8的目标检测模型为例：

![YOLOv8性能表现](https://img-blog.csdnimg.cn/74ef3c83cb894f5cbf2bf4621cfdb829.png)

YOLOv8的分割模型也提供了5中不同大小的模型选择：

![YOLOv8性能表现](https://img-blog.csdnimg.cn/4c962e0828174ba29545bfba2c41e5f4.png)

### 参考资料:

- <https://zhuanlan.zhihu.com/p/161439809>
- <https://zhuanlan.zhihu.com/p/135980432>
- <https://blog.csdn.net/u014380165/article/details/81273343>
- <https://blog.csdn.net/weixin_44791964/article/details/106533581>
- <https://blog.csdn.net/weixin_38353277/article/details/128930304>
- <https://blog.csdn.net/wjinjie/article/details/107509243>